{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a sigmoid neural network model to recognize digit number.\n",
    "##### using batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid gradient\n",
    "$${g}'(z)=\\frac{d}{dz}g(z)=g(z)(1-g(z))$$\n",
    "where\n",
    "$$sigmoid(z)=g(z)=\\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function with regularization\n",
    "$$J(\\theta )) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K}[-y_{k}^{(i)}log((h_\\theta(x^{(i)}))_{k}) - (1-y_{k}^{(i)})log(1-(h_{\\theta}(x^{(i)}))_{k})] + \\frac{\\lambda }{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_{l}}\\sum_{j=1}^{s_{l+1}}(\\theta_{ji}^{(l)})^{2}$$\n",
    "\n",
    "- Training set is $${(x^{1}, y^{1}), (x^{2}, y^{2}), (x^{3}, y^{3}) ... (x^{m}, y^{m})}$$\n",
    "- L = number of layers in the network\n",
    "- S<sub>l</sub> = number of units (not counting bias unit) in layer l \n",
    "- K is number of units in output layer S<sub>L</sub> = K\n",
    "-  $\\theta^{l}_{ji}$ is the weight from the ith unit of layer l to jth unit of layer l+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO complement explanation of neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    return sigmoid(z)*(1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initWeight(numUnits):\n",
    "    w = []\n",
    "    epsilon_init = 0.01\n",
    "    for i in range(len(numUnits)-1):\n",
    "        w.append(np.random.random((numUnits[i] + 1, numUnits[i+1]))*epsilon_init)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def NNcost(y, y_pre, theta, lamb):\n",
    "    m = len(y)\n",
    "    L = len(theta)\n",
    "    regular = 0\n",
    "    # calculate regularization term\n",
    "    for i in range(L):\n",
    "        # bias weight not included\n",
    "        regular += np.sum(np.square(theta[i][:,1:])) \n",
    "    regular = regular * lamb / m\n",
    "    cost = np.sum(- y * np.log(y_pre) - (1 - y) * np.log(1 - y_pre))/m + regular\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient(x, y, theta, lamb):\n",
    "    # number of theta\n",
    "    L = len(theta)\n",
    "    # number of data sets\n",
    "    m = len(x)\n",
    "    # save every a and z for backpropagation\n",
    "    a = []\n",
    "    z = []\n",
    "    current_a = x\n",
    "    current_z = 0\n",
    "    for i in range(L):\n",
    "        # add bias to input\n",
    "        current_a = np.c_[np.ones(m),current_a]\n",
    "        a.append(current_a)\n",
    "        current_z = np.dot(current_a , theta[i])\n",
    "        z.append(current_z)\n",
    "        current_a = sigmoid(current_z)\n",
    "    \n",
    "    # value of cost function\n",
    "    cost = NNcost(y, current_a, theta, lamb)\n",
    "    \n",
    "    # calculate the gradient of theta for updating\n",
    "    gradient = []\n",
    "    error = []\n",
    "    l = L-1\n",
    "    current_error = current_a - y\n",
    "    error.append(current_error)\n",
    "    delta = []\n",
    "    while l > 0:\n",
    "        current_error = np.dot(current_error,np.transpose(theta[l])) * sigmoidGradient(np.c_[np.ones(len(z[l-1])),z[l-1]])\n",
    "        current_error = current_error[:,1:]\n",
    "        error.append(current_error)\n",
    "        l -= 1\n",
    "        \n",
    "    for j in range(L):\n",
    "        delta.append(np.dot(np.transpose(a[j]),error[L-1-j]))\n",
    "    \n",
    "    theta_temp = [np.c_[np.zeros(len(t)),t[:,1:]] for t in theta]\n",
    "    theta_grad = []\n",
    "    \n",
    "    for i in range(L):\n",
    "        theta_grad.append(delta[i]/m + lamb/m * theta_temp[i])\n",
    "    \n",
    "    return (cost, theta_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateTheta(alpha, theta, theta_grad):\n",
    "    # batch gradient descent\n",
    "    for i in range(len(theta)):\n",
    "        theta[i] = theta[i] - alpha*theta_grad[i]\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(x, y, theta, alpha, lamb, max_cost, max_iter):\n",
    "    cost = 1\n",
    "    k = 0\n",
    "    while cost > max_cost and k < max_iter:\n",
    "        cost,theta_grad = gradient(x,y,theta,lamb)\n",
    "        theta = updateTheta(alpha, theta, theta_grad)\n",
    "        k = k + 1\n",
    "    return theta, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(x, theta):\n",
    "    # number of layers\n",
    "    L = len(theta)\n",
    "    # number of data sets\n",
    "    m = len(x)\n",
    "    a = x\n",
    "    for i in range(L):\n",
    "        # add bias to input\n",
    "        a = np.c_[np.ones(m),a]\n",
    "        z = np.dot(a , theta[i])\n",
    "        a = sigmoid(z)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oneHot(m):\n",
    "    n = len(np.unique(m))\n",
    "    r = np.zeros((len(m), n))\n",
    "    for i in range(len(m)):\n",
    "        r[i,m[i]] = 1\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data from https://www.kaggle.com/c/digit-recognizer\n",
    "df_train=pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = df_train[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = df_train.drop(\"label\", axis=1)/256.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trains y_train to one hot format\n",
    "y_train = oneHot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize theta\n",
    "theta = initWeight([784, 40, 10])\n",
    "alpha = 0.2\n",
    "lamb = 1\n",
    "max_cost = 0.01\n",
    "max_iter = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "theta, cost = train(X_train[:10000], y_train[:10000], theta, alpha, lamb, max_cost, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict the result of test set\n",
    "y_pre = predict(X_train[10000:12000], theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90749999999999997"
      ]
     },
     "execution_count": 877,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.round(y_pre), y_train[10000:12000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
